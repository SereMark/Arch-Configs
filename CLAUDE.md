# ðŸ§  CLAUDE.md - Optimized Memory Reference

## ðŸ–¥ï¸ SYSTEM SPECS [CRITICAL]
- **Arch Linux** | AMD Ryzen 5 3600 | NVIDIA RTX 2070 (8GB VRAM)
- **RAM**: 15.54 GiB | **GPU**: CUDA-capable | **Setup**: Single machine only

## ðŸš€ IMMEDIATE ACTIONS
```bash
source venv/bin/activate          # ALWAYS activate venv first
pip install -r requirements.txt   # New projects only
```

## âš¡ CORE DIRECTIVES
1. **SURGICAL PRECISION** â†’ Touch minimal code only
2. **ROOT CAUSE ONLY** â†’ Never mask symptoms  
3. **HUMAN-LIKE CODE** â†’ Readable > Clever
4. **HONEST ASSESSMENT** â†’ Never sugar-coat limitations

## ðŸŽ¯ PYTHON AI CHECKLIST
- [ ] Virtual environment activated
- [ ] Type hints for tensors: `torch.Tensor[batch, seq, dim]`
- [ ] Device handling: `device = 'cuda' if torch.cuda.is_available() else 'cpu'`
- [ ] Random seeds: `torch.manual_seed(42)` 
- [ ] Use `logging` not `print()`
- [ ] GPU memory monitoring (8GB limit)
- [ ] Batch size optimization for RTX 2070
- [ ] FP16/mixed precision when possible

## ðŸ”§ DEVELOPMENT WORKFLOW
1. **UNDERSTAND** â†’ Read codebase first
2. **MINIMAL SCOPE** â†’ Solve exactly what's asked
3. **ANALYZE** â†’ Check if fix already exists
4. **IMPLEMENT** â†’ Targeted root-cause changes
5. **VALIDATE** â†’ Test across data samples
6. **DOCUMENT** â†’ Update `dev-notes.md` locally

## ðŸ›¡ï¸ MEMORY CONSTRAINTS
- **System RAM**: 15GB total â†’ Keep 3GB buffer
- **GPU VRAM**: 8GB total â†’ Monitor with `nvidia-smi`
- **Batch sizing**: Start small, scale up
- **Checkpointing**: Frequent saves (single machine risk)

## âš ï¸ ANTI-PATTERNS [NEVER DO]
- âŒ Band-aid fixes or error suppression
- âŒ Hardcoded device specs (detect dynamically)
- âŒ TODO comments or status markers
- âŒ Breaking existing functionality
- âŒ Over-engineering solutions
- âŒ Ignoring GPU memory leaks

## ðŸ“ CODE STANDARDS
- **Self-documenting**: Variable names > comments
- **Minimal comments**: Only for complex algorithms
- **Debug logging**: Keep structured `logger.debug()`, remove `print()`
- **Follow patterns**: Match existing codebase style
- **Explicit > Implicit**: Clear code over shortcuts

## ðŸ¥· STEALTH PRACTICES
- Files: `dev-notes.md`, `debug-notes.md`, `dev.md`
- Documentation: Write as human developer
- Gitignore: Add as "Development notes"
- No AI mentions in any output

## ðŸŽ® QUICK COMMANDS
```python
# CUDA Check
import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
if device.type == 'cuda':
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

# Memory Monitoring
def get_gpu_memory():
    return torch.cuda.memory_allocated() / 1e9, torch.cuda.memory_reserved() / 1e9

# Batch Size Finder
def find_optimal_batch_size(model, sample_input, max_batch=128):
    batch = 1
    while batch <= max_batch:
        try:
            _ = model(sample_input.repeat(batch, 1, 1))
            batch *= 2
        except RuntimeError as e:
            if "out of memory" in str(e):
                torch.cuda.empty_cache()
                return batch // 2
            raise e
    return batch
```

## ðŸ”„ PROBLEM-SOLVING MATRIX
| Symptom | Check First | Likely Fix |
|---------|------------|------------|
| OOM Error | Batch size, GPU memory | Reduce batch, gradient accumulation |
| Slow Training | Device placement, FP16 | `.cuda()`, mixed precision |
| Reproducibility | Random seeds | Set all seeds (torch, numpy, random) |
| Model Loading | VRAM usage | Clear cache, load in chunks |

## ðŸ“Œ REMEMBER
- **DO EXACTLY** what's asked - nothing more, nothing less
- **EDIT > CREATE** - Never create files unless necessary
- **NO DOCS** unless explicitly requested
- **LOCAL DEV** - All work happens on this single machine